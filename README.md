# Bilibili ç¡¬æ ¸ä¼šå‘˜ç­”é¢˜ Benchmark æ”¶é›†å·¥å…·

è‡ªåŠ¨æ”¶é›† Bilibili ç¡¬æ ¸ä¼šå‘˜ç­”é¢˜æ•°æ®ï¼Œç”Ÿæˆ LLM Benchmark æ•°æ®é›†ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ”„ **æ¸è¿›å¼æ•°æ®æ”¶é›†**ï¼šå¤šæ¬¡è¿è¡Œç´¯ç§¯æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
- ğŸ¯ **æ™ºèƒ½ç­”é¢˜ç­–ç•¥**ï¼šå·²çŸ¥ç­”æ¡ˆæ—¶æ•…æ„é€‰é”™ï¼Œé¿å…é€šè¿‡ 60 åˆ†
- ğŸ“Š **å®Œæ•´æ•°æ®è·Ÿè¸ª**ï¼šè®°å½•æ¯ä¸ªé€‰é¡¹çš„çŠ¶æ€ï¼ˆæ­£ç¡®/é”™è¯¯/æœªçŸ¥ï¼‰
- ğŸ¤— **HuggingFace å…¼å®¹**ï¼šå¯¼å‡ºä¸ºæ ‡å‡†çš„ datasets æ ¼å¼
- ğŸ—ï¸ **ä¸“ä¸šæ¶æ„**ï¼šæ¸…æ™°çš„åˆ†å±‚è®¾è®¡ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

## ğŸš€ å¿«é€Ÿå¼€å§‹

**è¦æ±‚**ï¼šPython 3.10+ï¼ˆæ¨è 3.11 ~ 3.14ï¼‰

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… uvï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…é¡¹ç›®ä¾èµ–
uv sync
```

### 2. é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# OpenAI å…¼å®¹ API é…ç½®
OPENAI_BASE_URL=https://api.deepseek.com/v1
OPENAI_MODEL=deepseek-chat
OPENAI_API_KEY=your_api_key_here

# ç­”é¢˜é…ç½®
MAX_QUESTIONS=100
SAFETY_THRESHOLD=55

# æ•°æ®é…ç½®
BENCHMARK_VERSION=v1
```

### 3. è¿è¡Œ

#### ç­”é¢˜æ¨¡å¼ï¼ˆæ”¶é›†æ•°æ®ï¼‰

```bash
uv run python -m bili-hardcore-benchmark.main
```

ç¨‹åºä¼šï¼š
1. æ˜¾ç¤ºäºŒç»´ç ï¼Œæ‰«ç ç™»å½• Bç«™
2. éªŒè¯ç”¨æˆ·ç­‰çº§ï¼ˆéœ€ 6 çº§ï¼‰
3. è‡ªåŠ¨ç­”é¢˜å¹¶æ”¶é›†æ•°æ®
4. å®æ—¶ä¿å­˜åˆ° `benchmark_data/questions_raw.json`

**å¯å¤šæ¬¡è¿è¡Œ**ï¼Œæ¯æ¬¡è¿è¡Œéƒ½ä¼šç´¯ç§¯æ›´å¤šé¢˜ç›®æ•°æ®ã€‚

#### å¯¼å‡ºæ¨¡å¼ï¼ˆç”Ÿæˆ HuggingFace æ ¼å¼ï¼‰

```bash
uv run python -m bili-hardcore-benchmark.export
```

å¯¼å‡ºå®Œæ•´é¢˜ç›®ï¼ˆå·²çŸ¥æ­£ç¡®ç­”æ¡ˆï¼‰ä¸ºï¼š
- `benchmark_data/benchmark_v1/` - HuggingFace Arrow æ ¼å¼
- `benchmark_data/benchmark_v1.jsonl` - JSONL æ ¼å¼

#### å¯è§†åŒ–æ¨¡å¼ï¼ˆæŸ¥çœ‹æ”¶é›†è¿›åº¦ï¼‰

ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `benchmark_data/dashboard.html`ï¼Œå®ƒä¼šè‡ªåŠ¨è¯»å–åŒç›®å½•ä¸‹çš„ `questions_raw.json` å¹¶ç”Ÿæˆäº¤äº’å¼ä»ªè¡¨ç›˜ã€‚

ä»ªè¡¨ç›˜åŠŸèƒ½åŒ…æ‹¬ï¼š
- æ˜¾ç¤ºæ¯é“é¢˜ç›®çš„é€‰é¡¹çŠ¶æ€ï¼ˆæ­£ç¡®/é”™è¯¯/æœªçŸ¥ï¼‰
- æŒ‰åˆ†åŒºåˆ†ç»„å±•ç¤ºçƒ­åŠ›å›¾
- åŒ…å«åˆ†åŒºæ¦‚è§ˆã€å®Œæˆæƒ…å†µç»Ÿè®¡ç­‰
- æ”¯æŒäº¤äº’å¼æ“ä½œï¼ˆç¼©æ”¾ã€æ‚¬åœæŸ¥çœ‹è¯¦æƒ…ç­‰ï¼‰

## ğŸ“ é¡¹ç›®æ¶æ„

```
bili-hardcore-benchmark/
â”œâ”€â”€ application/              # åº”ç”¨å±‚ï¼šä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ models/              # æ•°æ®æ¨¡å‹ï¼ˆQuestion, Benchmarkï¼‰
â”‚   â””â”€â”€ services/            # ä¸šåŠ¡æœåŠ¡ï¼ˆç­”é¢˜ã€æ”¶é›†ã€å¯¼å‡ºï¼‰
â”œâ”€â”€ infrastructure/          # åŸºç¡€è®¾æ–½å±‚ï¼šæŠ€æœ¯å®ç°
â”‚   â”œâ”€â”€ bilibili/           # Bç«™ API å®¢æˆ·ç«¯ï¼ˆåŸºäº httpxï¼‰
â”‚   â”œâ”€â”€ ai/                 # AI æœåŠ¡ï¼ˆOpenAI å…¼å®¹ï¼‰
â”‚   â”œâ”€â”€ persistence/        # æ•°æ®æŒä¹…åŒ–å’Œå¯¼å‡º
â”‚   â””â”€â”€ config/             # é…ç½®ç®¡ç†ï¼ˆPydantic Settingsï¼‰
â”œâ”€â”€ common/                  # å…¬å…±ç»„ä»¶
â”‚   â”œâ”€â”€ exceptions.py       # å¼‚å¸¸ä½“ç³»
â”‚   â”œâ”€â”€ logging.py          # æ—¥å¿—é…ç½®
â”‚   â””â”€â”€ types.py            # ç±»å‹å®šä¹‰
â”œâ”€â”€ container.py            # ä¾èµ–æ³¨å…¥å®¹å™¨
â”œâ”€â”€ main.py                 # ç­”é¢˜æ¨¡å¼å…¥å£
â””â”€â”€ export.py               # å¯¼å‡ºæ¨¡å¼å…¥å£
```

### è®¾è®¡åŸåˆ™

- **ä¸¤å±‚æ¶æ„**ï¼šåº”ç”¨å±‚ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰+ åŸºç¡€è®¾æ–½å±‚ï¼ˆæŠ€æœ¯å®ç°ï¼‰
- **ä¾èµ–æ³¨å…¥**ï¼šé€šè¿‡å®¹å™¨ç®¡ç†æ‰€æœ‰ä¾èµ–å…³ç³»
- **ç±»å‹å®‰å…¨**ï¼šå…¨é¡¹ç›®ç±»å‹æ³¨è§£ï¼Œæ”¯æŒ mypy strict æ¨¡å¼
- **æ¸è¿›å¼é‡æ„**ï¼šæ–°æ¶æ„ä¸æ—§ä»£ç å¹¶å­˜ï¼Œå¹³æ»‘è¿ç§»

## ğŸ“Š æ•°æ®æ ¼å¼

### ä¸­é—´æ ¼å¼ï¼ˆ`questions_raw.json`ï¼‰

å­˜å‚¨æ‰€æœ‰é¢˜ç›®çš„è¯¦ç»†çŠ¶æ€ï¼š

```json
{
  "version": "1.0",
  "updated_at": "2024-01-15T10:30:00",
  "questions": {
    "12345": {
      "id": "12345",
      "question": "ä»¥ä¸‹å“ªä¸ªä¸æ˜¯ç¼–ç¨‹è¯­è¨€ï¼Ÿ",
      "choices": ["Python", "Java", "HTML", "C++"],
      "correct_answer": 2,
      "attempts": 3,
      "last_attempt": "2024-01-15T10:25:00"
    },
    "67890": {
      "id": "67890",
      "question": "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
      "choices": ["1", "2", "3", "4"],
      "wrong_answers": [0, 2],
      "attempts": 2
    }
  }
}
```

**å­—æ®µè¯´æ˜**ï¼š
- `correct_answer`: æ­£ç¡®ç­”æ¡ˆç´¢å¼•ï¼ˆ0-basedï¼‰ã€‚ä¸€æ—¦è®¾ç½®ï¼Œé¢˜ç›®å³å®Œæ•´
- `wrong_answers`: å·²çŸ¥é”™è¯¯ç­”æ¡ˆåˆ—è¡¨ï¼ˆä»…åœ¨æ­£ç¡®ç­”æ¡ˆæœªçŸ¥æ—¶ä½¿ç”¨ï¼‰
- å•é€‰é¢˜é€»è¾‘ï¼š`correct_answer` å­˜åœ¨æ—¶ï¼Œå…¶ä»–é€‰é¡¹è‡ªåŠ¨ä¸ºé”™è¯¯

### HuggingFace æ ¼å¼

å¯¼å‡ºåçš„æ ‡å‡†æ ¼å¼ï¼ˆåªåŒ…å«å®Œæ•´é¢˜ç›®ï¼‰ï¼š

```json
{
  "id": "12345",
  "question": "ä»¥ä¸‹å“ªä¸ªä¸æ˜¯ç¼–ç¨‹è¯­è¨€ï¼Ÿ",
  "choices": ["Python", "Java", "HTML", "C++"],
  "answer": 2,
  "category": "computer_science"
}
```

## ğŸ”§ ä½¿ç”¨æ•°æ®é›†

### åŠ è½½æ•°æ®

```python
from datasets import load_dataset, load_from_disk

# æ–¹æ³•1: åŠ è½½ Arrow æ ¼å¼ï¼ˆæ¨èï¼Œé€Ÿåº¦å¿«ï¼‰
dataset = load_from_disk("benchmark_data/benchmark_v1")

# æ–¹æ³•2: åŠ è½½ JSONL æ ¼å¼ï¼ˆå…¼å®¹æ€§å¥½ï¼Œæ˜“åˆ†äº«ï¼‰
dataset = load_dataset("json", data_files="benchmark_data/benchmark_v1.jsonl")

print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬: {dataset[0]}")
```

### è¯„ä¼°æ¨¡å‹

```python
# è¯„ä¼°æ¨¡å‹
correct = 0
for item in dataset:
    # ä½¿ç”¨ä½ çš„æ¨¡å‹é¢„æµ‹
    predicted = your_model.predict(item['question'], item['choices'])
    if predicted == item['answer']:
        correct += 1

accuracy = correct / len(dataset) * 100
print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
```

## ğŸ¯ æ™ºèƒ½ç­”é¢˜ç­–ç•¥

1. **å·²å®Œæ•´é¢˜ç›®**ï¼ˆ`correct_answer` å·²çŸ¥ï¼‰ï¼š
   - éšæœºé€‰æ‹©é”™è¯¯ç­”æ¡ˆï¼ˆé¿å…é€šè¿‡ 60 åˆ†ï¼‰
   - ä¸è°ƒç”¨ AIï¼ˆèŠ‚çœæˆæœ¬ï¼‰

2. **éƒ¨åˆ†å·²çŸ¥é¢˜ç›®**ï¼ˆæœ‰ `wrong_answers`ï¼‰ï¼š
   - ä»æœªå°è¯•çš„é€‰é¡¹ä¸­éšæœºé€‰æ‹©
   - æˆ–ä½¿ç”¨ AI åœ¨å‰©ä½™é€‰é¡¹ä¸­åˆ¤æ–­

3. **å®Œå…¨æœªçŸ¥é¢˜ç›®**ï¼š
   - ä½¿ç”¨ AI åˆ¤æ–­

## ğŸ› ï¸ å¼€å‘

### å®‰è£…å¼€å‘ä¾èµ–

```bash
uv sync --extra dev
```

### ä»£ç è´¨é‡æ£€æŸ¥

```bash
# ç±»å‹æ£€æŸ¥
uv run mypy bili-hardcore-benchmark

# ä»£ç æ ¼å¼åŒ–
uv run black bili-hardcore-benchmark

# Linting
uv run ruff check bili-hardcore-benchmark
```

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
